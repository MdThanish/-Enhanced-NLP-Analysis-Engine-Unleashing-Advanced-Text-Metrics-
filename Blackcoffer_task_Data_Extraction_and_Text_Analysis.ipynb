{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lNMH1gfF3cF",
        "outputId": "3474f221-df66-44ae-82b0-0123be61adae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Moham\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Moham\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#import necessary packages\n",
        "import os\n",
        "import re\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_ID</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37</td>\n",
              "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38</td>\n",
              "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39</td>\n",
              "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40</td>\n",
              "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41</td>\n",
              "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   URL_ID                                                URL\n",
              "0      37  https://insights.blackcoffer.com/ai-in-healthc...\n",
              "1      38  https://insights.blackcoffer.com/what-if-the-c...\n",
              "2      39  https://insights.blackcoffer.com/what-jobs-wil...\n",
              "3      40  https://insights.blackcoffer.com/will-machine-...\n",
              "4      41  https://insights.blackcoffer.com/will-ai-repla..."
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_excel('C:/Thanish Projects/blackCoffer_nlp_task/Input.xlsx')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXXMNkOohRgr",
        "outputId": "0de27ec4-58a7-4a93-f0ff-e5df6ed9c1f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "can't get title of 44\n",
            "can't get title of 57\n",
            "can't get title of 144\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#read the url file into the pandas object\n",
        "df = pd.read_excel('C:/Thanish Projects/blackCoffer_nlp_task/Input.xlsx')\n",
        "\n",
        "#loop throgh each row in the df\n",
        "for index, row in df.iterrows():\n",
        "  url = row['URL']\n",
        "  url_id = row['URL_ID']\n",
        "\n",
        "  # make a request to url\n",
        "  header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
        "  try:\n",
        "    response = requests.get(url,headers=header)\n",
        "  except:\n",
        "    print(\"can't get response of {}\".format(url_id))\n",
        "\n",
        "  #create a beautifulsoup object\n",
        "  try:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  except:\n",
        "    print(\"can't get page of {}\".format(url_id))\n",
        "  #find title\n",
        "  try:\n",
        "    title = soup.find('h1').get_text()\n",
        "  except:\n",
        "    print(\"can't get title of {}\".format(url_id))\n",
        "    continue\n",
        "  #find text\n",
        "  article = \"\"\n",
        "  try:\n",
        "    for p in soup.find_all('p'):\n",
        "      article += p.get_text()\n",
        "  except:\n",
        "    print(\"can't get text of {}\".format(url_id))\n",
        "\n",
        "  #write title and text to the file\n",
        "  file_name = 'C:/Thanish Projects/blackCoffer_nlp_task/titleoftext/' + str(url_id) + '.txt'\n",
        "  with open(file_name, 'w', encoding='utf-8') as file:\n",
        "    file.write(title + '\\n' + article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1tRdSv8ErMOm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Directories\n",
        "text_dir = \"C:/Thanish Projects/blackCoffer_nlp_task/titleoftext\"\n",
        "stopwords_dir = \"C:/Thanish Projects/blackCoffer_nlp_task/StopWords\"\n",
        "sentment_dir = \"C:/Thanish Projects/blackCoffer_nlp_task/MasterDictionary\"\n",
        "\n",
        "# load all stop wors from the stopwords directory and store in the set variable\n",
        "stop_words = set()\n",
        "for files in os.listdir(stopwords_dir):\n",
        "  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:\n",
        "    stop_words.update(set(f.read().splitlines()))\n",
        "\n",
        "# load all text files  from the  directory and store in a list(docs)\n",
        "docs = []\n",
        "for text_file in os.listdir(text_dir):\n",
        "  with open(os.path.join(text_dir, text_file), 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "#tokenize the given text file\n",
        "    words = word_tokenize(text)\n",
        "# remove the stop words from the tokens\n",
        "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "# add each filtered tokens of each file into a list\n",
        "    docs.append(filtered_text)\n",
        "# store positive, Negative words from the directory\n",
        "pos=set()\n",
        "neg=set()\n",
        "\n",
        "for files in os.listdir(sentment_dir):\n",
        "  if files =='positive-words.txt':\n",
        "    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n",
        "      pos.update(f.read().splitlines())\n",
        "  else:\n",
        "    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n",
        "      neg.update(f.read().splitlines())\n",
        "\n",
        "# now collect the positive  and negative words from each file\n",
        "# calculate the scores from the positive and negative words \n",
        "positive_words = []\n",
        "Negative_words =[]\n",
        "positive_score = []\n",
        "negative_score = []\n",
        "polarity_score = []\n",
        "subjectivity_score = []\n",
        "\n",
        "#iterate through the list of docs\n",
        "for i in range(len(docs)):\n",
        "  positive_words.append([word for word in docs[i] if word.lower() in pos])\n",
        "  Negative_words.append([word for word in docs[i] if word.lower() in neg])\n",
        "  positive_score.append(len(positive_words[i]))\n",
        "  negative_score.append(len(Negative_words[i]))\n",
        "  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
        "  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F8RaMuD_EnQQ"
      },
      "outputs": [],
      "source": [
        "# Directories\n",
        "text_dir = \"C:/Thanish Projects/blackCoffer_nlp_task/titleoftext\"\n",
        "stopwords_dir = \"C:/Thanish Projects/blackCoffer_nlp_task/StopWords\"\n",
        "\n",
        "# Load all stopwords from the stopwords directory and store in the stopwords_set variable\n",
        "stopwords_set = set()\n",
        "for filename in os.listdir(stopwords_dir):\n",
        "    with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
        "        stopwords_set.update(set(f.read().splitlines()))\n",
        "\n",
        "# Function to measure readability metrics\n",
        "def measure(file):\n",
        "    with open(os.path.join(text_dir, file), 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        \n",
        "    # Remove punctuations \n",
        "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
        "    \n",
        "    # Tokenize the given text file\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords from the tokens\n",
        "    filtered_text = [word for word in words if word.lower() not in stopwords_set]\n",
        "    \n",
        "    # Complex words having syllable count greater than 2\n",
        "    complex_words = [word for word in filtered_text if len(word) > 2]\n",
        "    \n",
        "    # Syllable count per word\n",
        "    syllable_count = sum(1 for word in filtered_text for letter in word if letter.lower() in 'aeiou')\n",
        "    \n",
        "    # Average sentence length\n",
        "    avg_sentence_len = len(filtered_text)\n",
        "    \n",
        "    # Percentage of complex words\n",
        "    percent_complex_words = len(complex_words) / len(filtered_text) if len(filtered_text) > 0 else 0\n",
        "    \n",
        "    # Fog Index\n",
        "    fog_index = 0.4 * (avg_sentence_len + percent_complex_words)\n",
        "    \n",
        "    return avg_sentence_len, percent_complex_words, fog_index, len(complex_words), syllable_count\n",
        "\n",
        "# Lists to store results\n",
        "avg_sentence_length = []\n",
        "percent_complex_words = []\n",
        "fog_index = []\n",
        "complex_word_count = []\n",
        "avg_syllable_word_count = []\n",
        "\n",
        "# Iterate through each file\n",
        "for file in os.listdir(text_dir):\n",
        "    x, y, z, a, b = measure(file)\n",
        "    avg_sentence_length.append(x)\n",
        "    percent_complex_words.append(y)\n",
        "    fog_index.append(z)\n",
        "    complex_word_count.append(a)\n",
        "    avg_syllable_word_count.append(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4NElx7d94ICm"
      },
      "outputs": [],
      "source": [
        "def cleaned_words(file):\n",
        "    with open(os.path.join(text_dir, file), 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        words = [word for word in text.split() if word.lower() not in stopwords_set]\n",
        "        length = sum(len(word) for word in words)\n",
        "        average_word_length = length / len(words) if len(words) > 0 else 0\n",
        "    return len(words), average_word_length\n",
        "\n",
        "word_count = []\n",
        "average_word_length = []\n",
        "for file in os.listdir(text_dir):\n",
        "  x, y = cleaned_words(file)\n",
        "  word_count.append(x)\n",
        "  average_word_length.append(y)\n",
        "\n",
        "\n",
        "# To calculate Personal Pronouns mentioned in the text, we use regex to find \n",
        "# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n",
        "#  so that the country name US is not included in the list.\n",
        "def count_personal_pronouns(file):\n",
        "  with open(os.path.join(text_dir, file), 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
        "    count = 0\n",
        "    for pronoun in personal_pronouns:\n",
        "      count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n",
        "  return count\n",
        "\n",
        "pp_count = []\n",
        "for file in os.listdir(text_dir):\n",
        "  x = count_personal_pronouns(file)\n",
        "  pp_count.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mXsnVluZ9TG3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved successfully to 'Output_Data.csv'\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate Percentage_of_Complex_words and Fog_Index\n",
        "def calculate_complexity(file):\n",
        "    with open(os.path.join(text_dir, file), 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuations\n",
        "        words = word_tokenize(text)  # Tokenize the text\n",
        "        filtered_text = [word for word in words if word.lower() not in stopwords_set]  # Remove stopwords\n",
        "        complex_words = [word for word in filtered_text if len(word) > 2]  # Complex words (syllable count > 2)\n",
        "        percent_complex_words = len(complex_words) / len(filtered_text) if len(filtered_text) > 0 else 0  # Percentage of complex words\n",
        "        avg_sentence_length = len(filtered_text)  # Average sentence length\n",
        "        fog_index = 0.4 * (avg_sentence_length + percent_complex_words)  # Fog Index\n",
        "    return percent_complex_words, fog_index\n",
        "\n",
        "# Lists to store results\n",
        "Percentage_of_Complex_words = []\n",
        "Fog_Index = []\n",
        "\n",
        "# Iterate through each file to calculate Percentage_of_Complex_words and Fog_Index\n",
        "for file in os.listdir(text_dir):\n",
        "    pcw, fog = calculate_complexity(file)\n",
        "    Percentage_of_Complex_words.append(pcw)\n",
        "    Fog_Index.append(fog)\n",
        "\n",
        "# Read the output data structure from Excel\n",
        "output_df = pd.read_excel('C:/Thanish Projects/blackCoffer_nlp_task/Output Data Structure.xlsx')\n",
        "\n",
        "# URL_ID 44, 57, 144 does not exist, i.e., the page does not exist and throws a 404 error\n",
        "# Drop these rows from the table\n",
        "output_df.drop([7, 20, 107], axis=0, inplace=True)\n",
        "\n",
        "# Define the required parameters\n",
        "variables = [positive_score,\n",
        "             negative_score,\n",
        "             polarity_score,\n",
        "             subjectivity_score,\n",
        "             avg_sentence_length,\n",
        "             Percentage_of_Complex_words,\n",
        "             Fog_Index,\n",
        "             complex_word_count,\n",
        "             word_count,\n",
        "             avg_syllable_word_count,\n",
        "             pp_count,\n",
        "             average_word_length]\n",
        "\n",
        "for i, var in enumerate(variables):\n",
        "    if len(var) != len(output_df):\n",
        "        print(f\"Length mismatch for variable {i+2} ({output_df.columns[i+2]}): Expected {len(output_df)} values, but got {len(var)}\")\n",
        "    else:\n",
        "        output_df.iloc[:, i+2] = var\n",
        "\n",
        "# Add average_word_length to the DataFrame\n",
        "output_df['AVG WORD LENGTH'] = average_word_length\n",
        "output_df['PERSONAL PRONOUNS'] = pp_count\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "try:\n",
        "    output_df.to_csv('C:/Thanish Projects/blackCoffer_nlp_task/Output_Data.csv', index=False)\n",
        "    print(\"Data saved successfully to 'Output_Data.csv'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the data: {e}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
